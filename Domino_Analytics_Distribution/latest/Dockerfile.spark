FROM quay.io/domino/base:Ubuntu18_DAD_Py3.8_R4.0-20210503_noCUDA

#######################################################
#                Add PySpark and Hadoop               #
#######################################################
# Taken from https://docs.dominodatalab.com/en/latest/reference/spark/on_demand_spark/Configuring_prerequisites.html
### Clear any existing PySpark install that may exist
### Omit if you know the environment does not have PySpark
RUN pip uninstall pyspark &>/dev/null

### Install PySpark matching the Spark version of your base image
### Modify the version below as needed
RUN pip install pyspark==3.1.1

### Set SPARK_HOME on the driver to point to the version installed by pyspark
RUN \
  SPARK_HOME=$(pip show pyspark | grep "Location" | awk '{print $2}')/pyspark && \
  chown -R ubuntu:ubuntu ${SPARK_HOME} && \
  echo "export SPARK_HOME=${SPARK_HOME}" >> /home/ubuntu/.domino-defaults && \
  echo "export PATH=\$PATH:${SPARK_HOME}/bin" >> /home/ubuntu/.domino-defaults

### Optionally copy spark-submit to spark-submit.sh to be able to run from Domino jobs
RUN spark_submit_path=$(which spark-submit) && \
    cp ${spark_submit_path} ${spark_submit_path}.sh

### Hadoop does not come with the required binaries for AWS access so we add them
### hadoop-aws.jar must match the hadoop-common.jar version of your install
RUN \
  SPARK_HOME=$(pip show pyspark | grep "Location" | awk '{print $2}')/pyspark && \
  rm -rf ${SPARK_HOME}/hadoop-aws* && \
  rm -rf ${SPARK_HOME}/aws-java-sdk* && \
  curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar --output ${SPARK_HOME}/jars/hadoop-aws-3.2.0.jar && \
  curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.11.999/aws-java-sdk-1.11.999.jar --output ${SPARK_HOME}/jars/aws-java-sdk-1.11.999.jar